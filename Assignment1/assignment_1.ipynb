{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ced82c6063718e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Data Mining and Machine Learning - Assignment 1**\n",
    "\n",
    "# Question 1 - NOx Study\n",
    "\n",
    "Modelling of $LNOx$ concentration as function of other variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24d6bb1624e254e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.019052Z",
     "start_time": "2024-04-11T12:12:08.516173Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Import of used libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2302dd7109b3d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.041649Z",
     "start_time": "2024-04-11T12:12:09.020061Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Import of the dataset\n",
    "q1_pd = pd.read_csv('NOxEmissions.csv')\n",
    "q1_pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a6fb9a5d870a7ad5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (a) - Data Pre-processing\n",
    "\n",
    "In the Pre-Processing stage, the following actions were made:\n",
    "\n",
    "- **Missing data:** No missing data found in the dataset\n",
    "- **Duplicates:** No duplicates were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4745751921d43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.059543Z",
     "start_time": "2024-04-11T12:12:09.041649Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# (a) - Pre-processing\n",
    "\n",
    "# Check if missing/duplicated/Invalid data is present in the dataset\n",
    "\n",
    "## Missing data\n",
    "print(f\"Number of missing data: {q1_pd.isnull().sum().sum()}\")\n",
    "## Duplicated data\n",
    "print(f\"Number of duplicated data: {q1_pd.duplicated().sum()}\")\n",
    "\n",
    "## Statistical Summary\n",
    "print(f\"===Statistical Summary===\\n{q1_pd.describe()}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803c6712dbeab988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.233277Z",
     "start_time": "2024-04-11T12:12:09.059543Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Check for outliers\n",
    "\n",
    "melted_data = pd.melt(q1_pd, value_vars=['LNOx', 'LNOxEm', 'sqrtWS'], var_name='Variable', value_name='Value')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(8, 5)) \n",
    "boxplot = sns.boxplot(x='Variable', y='Value', data=melted_data)\n",
    "boxplot.set_title('Distribution and Outliers of Data Variables')\n",
    "boxplot.set_ylabel('Value')\n",
    "boxplot.set_xlabel('Variable')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1256ab9b0f84f0a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (b) - Distribution of LNOx variable\n",
    "\n",
    "$LNOx$ appears to follow a normal distribution with a significant number of outliers on the left side (as shown by the previous box-plot).\n",
    "A left (negative) skew is also identified from the graph and by using the *Skewness* indicator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c442edf31689470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.686981Z",
     "start_time": "2024-04-11T12:12:09.233277Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# (b) - LNOx distribution\n",
    "\n",
    "lnox = q1_pd['LNOx']\n",
    "\n",
    "## Descriptive Stats\n",
    "range_lnox = lnox.max() - lnox.min()\n",
    "print(f\"Mean: {lnox.mean()}\\nMedian: {lnox.median()}\\nStandard Deviation: {lnox.std()}\\nVariance: {lnox.var()}\\nRange: {range_lnox}\\nSkewness: {lnox.skew()}\\nKurtosis: {lnox.kurt()}\")\n",
    "\n",
    "## Histogram plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(q1_pd['LNOx'], kde=True)\n",
    "plt.title('Histogram of LNOx')\n",
    "plt.xlabel('LNOx')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "stats.probplot(q1_pd['LNOx'], dist=\"norm\", plot=ax)\n",
    "ax.set_title(\"Q-Q Plot for LNOx Variable\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f58d92905fdceb4d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (c) - Linear Model of LNOx as fn. of LNOxEm, sqrtWS\n",
    "\n",
    "The $LNOx$ linear model is fitted below using a multiple linear regression model, $LNOx$ is the dependent variable, $LNOxEm$ and $sqrtWS$ are the independent variables.\n",
    "\n",
    "The OLS-regression results from the model shows that:\n",
    "\n",
    "- $R^2 = 0.663$, which means that the independent variables can explain about $66\\%$ of variability of $LNOx$.\n",
    "- The coefficients of the independent variables explains:\n",
    "    - $LNOxEm$: When this variable increases, $LNOx$ **increases** too by a factor of $\\approx 0.06$.\n",
    "    - $sqrtWS$: When the square root of wind speed increases, $LNOx$ gets **decreased** by a factor of $\\approx 1.01$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d275791b075fb916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.708773Z",
     "start_time": "2024-04-11T12:12:09.686981Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# (c) - LNOx linear model \n",
    "\n",
    "X = q1_pd[['LNOxEm', 'sqrtWS']]\n",
    "X = sm.add_constant(X)\n",
    "y = q1_pd['LNOx']\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b64e38ee6bdc45b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (d) - Relationship between dependent and independent variables\n",
    "\n",
    "In the Linear Regression model created above, the concentration of nitrogen close to a motorway ($LNOx$, the dependent variable) is influenced by:\n",
    "\n",
    "- The emission of $NOx$ of cars on the motorway ($LNOxEm$)\n",
    "- The square root of wind speed ($sqrtWS$)\n",
    "\n",
    "The results of the model shows that both (independent) variables are significant in determining the concentration of $NOx$. An increase of the wind speed ($sqrtWS$) tends to lower the concentration of $NOx$, probably because it would disperse the $NOx$ present in the air. \n",
    "On the other hand, $LNOxEm$ has a positive impact on the concentration of $NOx$. This likely means that when the volume of emissions of cars in the motorway increases, so does the $NOx$ concentration close to the motorway. However, this affects the concentration of nitrogen less than the wind does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0f08031ffff2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (e) - Prediction of LNOx given the indp. variables values\n",
    "\n",
    "Given the value of the emission of cars in the motorway ($LNOxEm = 7.5$) and wind speed ($sqrtWS = 1.3$) the estimated value for the concentration of pollution close to the motorway is $LNOx \\approx 4.55$. This means that given the amount of pollution the cars are making ($7.5$) and how fast the wind is blowing ($1.3$) the air pollution is expected to be around $4.55$.\n",
    "\n",
    "By consulting the data available, the prediction is close to the average concentration of pollution, suggesting that the prediction is within reasonable ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea42a699a6d5841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:09.977777Z",
     "start_time": "2024-04-11T12:12:09.708773Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# (e) - prediction using pre-defined values\n",
    "\n",
    "# LNOxEm = 7.5, sqrtWS = 1.3\n",
    "new_data = pd.DataFrame({'const': 1, 'LNOxEm': [7.5], 'sqrtWS': [1.3]})\n",
    "predicted_LNOx = model.predict(new_data)\n",
    "print(f\"The predicted LNOx value is: {predicted_LNOx[0]:.2f}\")\n",
    "\n",
    "# Distribution + predicted value\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(q1_pd['LNOx'], kde=True, color=\"skyblue\", label='LNOx Distribution')\n",
    "plt.axvline(x=predicted_LNOx[0], color='red', linestyle='--', label=f'Predicted LNOx for\\nLNOxEm=7.5, sqrtWS=1.3: {predicted_LNOx.iloc[0]:.2f}')\n",
    "plt.legend()\n",
    "plt.title('Distribution of LNOx with Predicted Value Highlighted')\n",
    "plt.xlabel('LNOx')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3befdf965dd30d6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Question 2 - Airbag study\n",
    "\n",
    "Modelling the probability of surviving a crash given 7 variables, using a *Generalized Linear Model*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1326dc400c061d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:10.021941Z",
     "start_time": "2024-04-11T12:12:09.977777Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Q2 - Dataset load + visualization\n",
    "q2_pd = pd.read_csv('nassCDS.csv')\n",
    "q2_pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f571b90c38f6d5cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (a) - Data Pre-Processing\n",
    "\n",
    "The dataset has $154$ missing values. Considering that the dataset is very numerous, to avoid having problems with predictions, all the records with missing values are dropped. \n",
    "\n",
    "As GLM will be applied to predict the *dead* variable, a histogram is used to check for an imbalance between the two groups of the *dead* variable (fatal / non-fatal crashes).\n",
    "The graph below shows that there is a big imbalance between the data for fatal/non-fatal crashes. To address this problem, the data of fatal crashes will be upsampled in order to match the number of data related to non-fatal crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa3cebb46f194cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:10.067559Z",
     "start_time": "2024-04-11T12:12:10.021941Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2.1 - Dataset load and Pre-Processing\n",
    "\n",
    "## Missing data\n",
    "print(f\"Number of missing data: {q2_pd.isnull().sum().sum()}\")\n",
    "## Duplicated data\n",
    "print(f\"Number of duplicated data: {q2_pd.duplicated().sum()}\")\n",
    "\n",
    "## Statistical Summary\n",
    "print(f\"===Statistical Summary===\\n{q2_pd.describe()}\")\n",
    "\n",
    "# Drop all the records with missing data\n",
    "q2_clean_pd = q2_pd.dropna()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e393a64f914ab720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:10.175145Z",
     "start_time": "2024-04-11T12:12:10.067559Z"
    }
   },
   "source": [
    "# Check for data imbalance / plot\n",
    "dead_count = q2_clean_pd.groupby(\"dead\")[\"dead\"].count()\n",
    "print(dead_count)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=dead_count)\n",
    "plt.xlabel('Dead var.')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of the Dead variable')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff52f44a5a7c2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:10.229868Z",
     "start_time": "2024-04-11T12:12:10.175145Z"
    }
   },
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# up-sampling the dataset\n",
    "\n",
    "q2_df_minority = q2_clean_pd[(q2_clean_pd['dead'] == 'dead')]\n",
    "q2_df_majority = q2_clean_pd[(q2_clean_pd['dead'] == 'alive')]\n",
    "\n",
    "q2_df_minority_upsampled = resample(q2_df_minority, replace=True, n_samples=dead_count[0], random_state=42)\n",
    "q2_df_minority_upsampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "q2_df_upsampled = pd.concat([q2_df_minority_upsampled, q2_df_majority])\n",
    "\n",
    "dead_count2 = q2_df_upsampled.groupby(\"dead\")[\"dead\"].count()\n",
    "print(\"==Upsampled dataset count==\")\n",
    "print(dead_count2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "658f2e94c20bda47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (b) - is Seat Belt var. independent related to the dead var.\n",
    "\n",
    "Considering the following hypothesis:\n",
    "\n",
    "- $H_0$: seatbelt use is independent of whether a passenger survives or not\n",
    "- $H_1$: seatbelt use is not independent of whether a passenger survives or not\n",
    "\n",
    "The Seat Belt is not independent whether the passenger survives or not, which is checked by the $\\chi^2$ test between the seatbelt and \"dead\" data. Having the P-value very close to zero rejects the *Null Hypothesis* $H_0$, indicating that the usage of the seatbelt is likely to influence the survival outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4deb83511161678e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:10.246564Z",
     "start_time": "2024-04-11T12:12:10.229868Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "table = pd.crosstab(q2_clean_pd['seatbelt'], q2_clean_pd['dead'])\n",
    "print(table)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, _, _ = chi2_contingency(table)\n",
    "print(f\"Chi-square Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5b10e9f84a4ad176",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## (c) - Mean age difference between inj. severity groups\n",
    "\n",
    "The box plot below shows that there is a significant difference of the mean age between the 5 groups of injury. The difference is more evident with the \"Killed\" group, where the median is significantly greater than the other, reaching a value of $\\approx 40$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc0c358011dbc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:10.429402Z",
     "start_time": "2024-04-11T12:12:10.246564Z"
    }
   },
   "source": [
    "# Box plot visualization\n",
    "q2_filtered = q2_clean_pd[q2_clean_pd['injSeverity'] < 5]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='injSeverity', y='ageOFocc', data=q2_filtered)\n",
    "plt.title('Age Distribution by Injury Severity')\n",
    "plt.xlabel('Injury Severity')\n",
    "plt.ylabel('Age of Occupant')\n",
    "plt.xticks([0, 1, 2, 3, 4], ['None', 'Possible Injury', 'No Incapacity', 'Incapacity', 'Killed'])\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7c68131da4441c67",
   "metadata": {},
   "source": [
    "## (d) - GLM for prob. of surviving\n",
    "\n",
    "The GLM will try to predict the probability of surviving as a function of *airbag*, *seatbelt*, *frontal*, *sex*, *ageOFocc*, *yearVeh*, *deploy* using **70%** to train the model and **30%** of the remaining  data to test it.\n",
    "\n",
    "Before training the model using 70% of the dataset over dispersion was checked by using the $OD = \\frac{\\text{pearson's } \\chi^2}{\\text{Df residuals}}$ and the scale value of the *Quasi-binomial* model.\n",
    "The test results suggests that there is not enough evidence to show that the model is inadequate.\n",
    "\n",
    "yearVeh was removed from the final formula for the GLM as it does not contribute enough to the result (P value).\n",
    "\n",
    "### Comment on GLM model performance\n",
    "\n",
    "The Binomial GLM model evaluation showed the following results:\n",
    "\n",
    "- $Accuracy = 0.6812 \\approx 68\\%$, this means that out of 100 predictions about a person surviving or not a crash, the model correctly predicts only 68 times.\n",
    "- $Sensitivity = 0.6847 \\approx 68\\%$, this indicator shows that for every 100 persons that actually survived the crash the model correctly predicts only 68 of them.\n",
    "- $Specificity = 0.6777 \\approx 68\\%$, means that for every 100 persons that did not manage to survive the crash the model only correctly identifies 68 of them.\n",
    "\n",
    "The results show that the model should be improved to better predict a fatal crash. One of the possible reasons for the low accuracy could be how the prediction, expressed as a decimal number from 0 to 1 get converted to a nominal value of either 'dead' or 'alive'. In this model if the value is $< 0.5$ then the prediction is labeled as 'dead', 'alive' is assigned otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d155c9ad8272e417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:12.201243Z",
     "start_time": "2024-04-11T12:12:10.429402Z"
    }
   },
   "source": [
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# First step - Check for over dispersion and define formula\n",
    "\n",
    "glm_formula = 'dead ~ C(airbag) + C(seatbelt) + C(frontal) + C(sex) + ageOFocc + yearVeh + C(deploy)'\n",
    "\n",
    "glm_model = glm(formula=glm_formula, data=q2_df_upsampled, family=sm.families.Binomial()).fit()\n",
    "glm_model_x2 = glm(formula=glm_formula, data=q2_df_upsampled, family=sm.families.Binomial()).fit(scale=\"X2\")\n",
    "\n",
    "dof = len(glm_model.resid_pearson) - glm_model.df_model - 1 \n",
    "pearson_over_dispersion = glm_model.pearson_chi2 / dof\n",
    "print(f\"Pearson's chi-2 / Df Residuals: {pearson_over_dispersion}\")\n",
    "print(f\"Quasi-binomial model scale: {glm_model_x2.scale}\")\n",
    "\n",
    "glm_model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f3737aabb098f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:12.872906Z",
     "start_time": "2024-04-11T12:12:12.202761Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Second step - 70/30 % separation, fit of the final GLM model, get the predictions\n",
    "\n",
    "# yearVeh is removed as is not significant\n",
    "glm_formula2 = 'dead ~ C(airbag) + C(seatbelt) + C(frontal) + C(sex) + ageOFocc + C(deploy)'\n",
    "\n",
    "# only relevant variables are obtained\n",
    "X = q2_df_upsampled[['airbag', 'seatbelt', 'frontal', 'sex', 'ageOFocc', 'deploy']]\n",
    "y = q2_df_upsampled['dead']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "q2_df_train = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "glm_eval = glm(formula=glm_formula2, data=q2_df_train, family=sm.families.Binomial()).fit()\n",
    "\n",
    "# Get the predictions for the fitted model\n",
    "y_pred = glm_eval.predict(X_test)\n",
    "y_pred_nominal = np.where(y_pred < 0.5, 'dead', 'alive') # convert predictions to nominal values of either 'dead' or 'alive'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78afa5bada6cb78e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:13.040883Z",
     "start_time": "2024-04-11T12:12:12.872906Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Third Step - Model Evaluation\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_nominal)\n",
    "cm = confusion_matrix(y_test, y_pred_nominal)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Sensitivity: {sensitivity:.4f}')\n",
    "print(f'Specificity: {specificity:.4f}')\n",
    "\n",
    "## Visualization\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Predicted Dead', 'Predicted Alive'], yticklabels=['Actual Dead', 'Actual Alive'])\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Actuals')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7cdd21af1ae4a19b",
   "metadata": {},
   "source": [
    "## (e) - Interpretation of the seatbelt and ageOFocc parameter\n",
    "\n",
    "In the trained model the coefficients are:\n",
    "\n",
    "- $\\text{seatbelt(none)} = -1.3971$, the negative parameter tells us that not using a seatbelt significantly decreases the chance to survive a crash. It is also the factor that decreases the probability of surviving the most between the other parameters.\n",
    "- $\\text{ageOFocc} = -0.0262$, this means that an older occupant will slightly decrease the probability of surviving. As the number is close to zero it means that compared to other parameters like *airbag* or *seatbelt* it will not contribute as much to the outcome of the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26394ffb30f90094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:13.068769Z",
     "start_time": "2024-04-11T12:12:13.040883Z"
    }
   },
   "source": [
    "# Summary of the trained model to consult the coef.\n",
    "glm_eval.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "46985567f4b067e1",
   "metadata": {},
   "source": [
    "## (f) - Predictions given a scenario\n",
    "\n",
    "By calculating the odds of not surviving using the formula $odds = \\frac{1-P}{P}$ (where $P$ is the probability of not surviving) the results are:\n",
    "\n",
    "- Scenario **1**:\n",
    "For this scenario where there is no airbag and belt, the odds for the occupant not surviving is $7.5$.\n",
    "- Scenario **2**:\n",
    "In this case where belt and airbag is present and deployed, the odds of the occupant not surviving are significantly lower with a value of just $0.2855$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be251d97cbdcb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:13.078872Z",
     "start_time": "2024-04-11T12:12:13.068769Z"
    }
   },
   "source": [
    "# Dataframe containing both scenarios\n",
    "scenarios = pd.DataFrame({\n",
    "    'airbag': ['none', 'airbag'],\n",
    "    'seatbelt': ['none', 'belted'],  \n",
    "    'frontal': [1, 1],\n",
    "    'sex': ['f', 'f'],        \n",
    "    'ageOFocc': [70, 70],  \n",
    "    'deploy': [1, 0]   \n",
    "})\n",
    "\n",
    "prob_surviving = glm_eval.predict(scenarios)\n",
    "odds_not_surviving = (1 - prob_surviving) / prob_surviving\n",
    "\n",
    "for i, odds in enumerate(odds_not_surviving, 1):\n",
    "    print(f\"Scenario {i}: Odds of not surviving= {odds:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbb4dd0bd115796",
   "metadata": {},
   "source": [
    "# Question 3 - Intl. Student Flow between countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a64754e830b8d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:13.377924Z",
     "start_time": "2024-04-11T12:12:13.078872Z"
    }
   },
   "source": [
    "# Import of the dataset\n",
    "q3_df = pd.read_excel('data_q3.xlsx')\n",
    "#q3_df # Commented to avoid cluttering the pdf"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a9f7641794f33d8d",
   "metadata": {},
   "source": [
    "## (a) - Data Pre-processing\n",
    "\n",
    "The provided dataset has 10 missing values (in the considered variables), to avoid problems records with missing values are dropped.\n",
    "\n",
    "As clustering algorithms will be applied, a dataset called *q3_scaled_df* is created, this dataset is preprocessed from the dataset *q3_cleaned_df* (that contains only variables of interest and no records with missing values) as follows:\n",
    "\n",
    "- All categorical and unique variables are removed. This is important as they can't be used to measure the Euclidean distance between data points.\n",
    "- All the numerical data is scaled so that they all contribute equally to the distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a8d7b92a7ee2e1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:13.389713Z",
     "start_time": "2024-04-11T12:12:13.379429Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Selection of column of interest\n",
    "q3_clean_df = q3_df[['InboundRatio', 'InternationalStudentsNO', 'KOFPoGI', 'KOFEcGI', 'KOFSoGI', 'ISCED5 Percentage', 'ISCED6 Percentage', 'ISCED7 Percentage', 'ISCED8 Percentage', 'top_50_count', 'top_100_count', 'top_500_count', 'top_1000_count', 'WESP', 'country_x']]\n",
    "\n",
    "## Missing data\n",
    "print(f\"Number of missing data: {q3_clean_df.isnull().sum().sum()}\")\n",
    "## Duplicated data\n",
    "print(f\"Number of duplicated data: {q3_clean_df.duplicated().sum()}\")\n",
    "\n",
    "## Drop of records with missing data\n",
    "q3_clean_df = q3_clean_df.dropna()\n",
    "\n",
    "## Drop categorical / unique values\n",
    "q3_scaled_df = q3_clean_df.drop(['WESP', 'country_x'], axis=1)\n",
    "\n",
    "## Scale the data (to apply K-Means)\n",
    "scaler = StandardScaler()\n",
    "q3_scaled_df = scaler.fit_transform(q3_scaled_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "61ca7d5854e63992",
   "metadata": {},
   "source": [
    "## (b) - K-Mean Cluster Analysis\n",
    "\n",
    "By applying using the *Elbow method* and *silhouette score* the ideal number of clusters looks to be $K = 3$.\n",
    "This is because in the first plot the elbow point appears to be where $K = 3$ as the next points looks to be in a line, and paired with the silhouette score plot it shows 3 as a potential candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92ca37e5ea088a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:14.073382Z",
     "start_time": "2024-04-11T12:12:13.389713Z"
    }
   },
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# KMeans analysis\n",
    "\n",
    "sse = []\n",
    "silhouette_coeff = []\n",
    "range_n_clusters = range(2, 16)\n",
    "for k in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(q3_scaled_df)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    score = silhouette_score(q3_scaled_df, kmeans.labels_)\n",
    "    silhouette_coeff.append(score)\n",
    "\n",
    "# Elbow Method\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=range_n_clusters, y=sse, marker=\"o\", dashes=False)\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Silhouette score\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=range_n_clusters, y=silhouette_coeff, marker=\"o\", dashes=False)\n",
    "plt.title('Silhouette Score for optimal K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "84f39015883e2cb8",
   "metadata": {},
   "source": [
    "## (c) - Agglomerative cluster analysis\n",
    "\n",
    "By using the dendrogram it seems that the best number of clusters is still $K = 3$. This is because the best \"cut\" appears to be at the third level (in the plot below, right to left). The cut was made at the third level as it is the only level where there is a significant distance to the upper level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b31fa50d64551795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:14.321359Z",
     "start_time": "2024-04-11T12:12:14.073382Z"
    }
   },
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Agglomerative cl. analysis\n",
    "linked = linkage(q3_scaled_df, 'ward')\n",
    "\n",
    "# Plotting the Dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='right', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Agglomerative Clustering Dendrogram')\n",
    "plt.ylabel('Sample index')\n",
    "plt.xlabel('Distance')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "53d2fc243e6e4408",
   "metadata": {},
   "source": [
    "## (d) - Conclusions\n",
    "\n",
    "By using the number of clusters $K = 3$ with both K-Means and Agglomerative clustering, the identified clusters contains significant differences in terms of university ranking, number of international students and globalization index.\n",
    "\n",
    "From the PCA-Reduced plot below it seems that the best cluster set was identified by the Agglomerative Clustering method as it isolated the USA inside his own cluster.\n",
    "\n",
    "As shown by the plots below, countries that are in the same cluster have a similar globalization index and number of international students. The only outlier is the United States, which has so many International Students and top ranking universities that in both K-Means and Aggl. clustering it is in his own cluster (along with the UK).\n",
    "\n",
    "The Number of international Students and top ranking universities seems to also be related with the WESP designation. Countries part of the clusters with more top ranking universities are considered Developed and only few developing, while the cluster with the least number of students and top universities has a majority of countries that are in the developing/in transition designation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "536abfa9ca99e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:14.723043Z",
     "start_time": "2024-04-11T12:12:14.321359Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply K-Means + Agglomerative clustering with identified K = 3\n",
    "# K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(q3_scaled_df)\n",
    "q3_clean_df['ClusterK'] = kmeans.labels_\n",
    "\n",
    "# Aggl. clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "agg_clustering.fit(q3_scaled_df)\n",
    "q3_clean_df['ClusterA'] = agg_clustering.labels_\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "q3_reduced = pca.fit_transform(q3_scaled_df)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11, 5))\n",
    "\n",
    "# K-Means Plot\n",
    "sns.scatterplot(x=q3_reduced[:, 0], y=q3_reduced[:, 1], hue=kmeans.labels_, palette='viridis', s=50, edgecolor='k', legend='full', ax=ax[0])\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax[0].scatter(centroids[:, 0], centroids[:, 1], marker='X', s=100, c='red', label='Centroids')\n",
    "ax[0].set_title('K-Means Clustering Results')\n",
    "ax[0].set_xlabel('Principal Component 1')\n",
    "ax[0].set_ylabel('Principal Component 2')\n",
    "ax[0].legend()\n",
    "\n",
    "# Aggl. Clustering Plot\n",
    "sns.scatterplot(x=q3_reduced[:, 0], y=q3_reduced[:, 1], hue=agg_clustering.labels_, palette='viridis', s=50, edgecolor='k', legend='full', ax=ax[1])\n",
    "ax[1].set_title('Agglomerative Clustering Results')\n",
    "ax[1].set_xlabel('Principal Component 1')\n",
    "ax[1].set_ylabel('Principal Component 2')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4583f5dc015f5ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:14.951991Z",
     "start_time": "2024-04-11T12:12:14.723043Z"
    }
   },
   "source": [
    "# Plot KOFEcGI vs Inbound Students\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=q3_clean_df, x='KOFEcGI', y='InternationalStudentsNO', hue='ClusterA', style='ClusterA', palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "plt.title('Economic Globalization vs. Number of International Students by Agg. Cluster')\n",
    "plt.xlabel('Economic Globalization Index (KOFEcGI)')\n",
    "plt.ylabel('Number of International Students')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Print the country name close to each point\n",
    "for i, point in q3_clean_df.iterrows():\n",
    "    plt.text(point['KOFEcGI']+0.02, point['InternationalStudentsNO'], str(point['country_x']), fontsize=9, ha='left')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dbd7752370cf2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:14.963703Z",
     "start_time": "2024-04-11T12:12:14.951991Z"
    }
   },
   "source": [
    "top_50_clusterA = q3_clean_df.groupby('ClusterA')['top_50_count'].sum().reset_index()\n",
    "top_100_clusterA = q3_clean_df.groupby('ClusterA')['top_100_count'].sum().reset_index()\n",
    "top_50_clusterK = q3_clean_df.groupby('ClusterK')['top_50_count'].sum().reset_index()\n",
    "top_100_clusterK = q3_clean_df.groupby('ClusterK')['top_100_count'].sum().reset_index()\n",
    "print(\"Agglomerative Clustering results\")\n",
    "print(f'Top 50 universities in each cluster\\n{top_50_clusterA}')\n",
    "print(f'Top 100 universities in each cluster\\n{top_100_clusterA}')\n",
    "print(\"K-Means Clustering results\")\n",
    "print(f'Top 50 universities in each cluster\\n{top_50_clusterK}')\n",
    "print(f'Top 100 universities in each cluster\\n{top_100_clusterK}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbfe13416092eb0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:12:15.275Z",
     "start_time": "2024-04-11T12:12:14.963703Z"
    }
   },
   "source": [
    "wesp_distributionK = q3_clean_df.groupby(['ClusterK', 'WESP']).size().reset_index(name='Country Count')\n",
    "wesp_distributionA = q3_clean_df.groupby(['ClusterA', 'WESP']).size().reset_index(name='Country Count')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11, 5))\n",
    "\n",
    "sns.barplot(data=wesp_distributionK, x='ClusterK', y='Country Count', hue='WESP', ax=ax[0])\n",
    "ax[0].set_title('Distribution by WESP Category using K-Means Cluster')\n",
    "ax[0].set_xlabel('K-Means Cluster')\n",
    "ax[0].set_ylabel('Number of Countries')\n",
    "ax[0].legend(title='WESP Category')\n",
    "\n",
    "sns.barplot(data=wesp_distributionA, x='ClusterA', y='Country Count', hue='WESP', ax=ax[1])\n",
    "ax[1].set_title('Distribution by WESP Category using Agglomerated Clustering')\n",
    "ax[1].set_xlabel('Agg. Cluster')\n",
    "ax[1].set_ylabel('Number of Countries')\n",
    "ax[1].legend(title='WESP Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
